{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c3a0b1ad-e6b3-4bc6-a907-5df2e4ed2d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_scatter import scatter_add\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "def format_pytorch_version(version):\n",
    "  return version.split('+')[0]\n",
    "\n",
    "TORCH_version = torch.__version__\n",
    "TORCH = format_pytorch_version(TORCH_version)\n",
    "\n",
    "def format_cuda_version(version):\n",
    "  return 'cu' + version.replace('.', '')\n",
    "\n",
    "CUDA_version = torch.version.cuda\n",
    "CUDA = format_cuda_version(CUDA_version)\n",
    "\n",
    "from torch_geometric.data import Data, Dataset#,DataLoader\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch import Tensor\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as Tr\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch.nn import Sequential as Seq, Linear, ReLU, Sigmoid\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, graph_files, file_name, transform=None, pre_transform=None):\n",
    "        super(GraphDataset, self).__init__()\n",
    "\n",
    "        self.graph_files = graph_files\n",
    "        self.file_name = file_name\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return self.graph_files\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return []\n",
    "\n",
    "    def get(self, idx):\n",
    "        data = torch.load(f'/users/santoshp/trackml_data/mockdata/{self.file_name}' + f'event{idx}.pyg')\n",
    "        print(f\"Loaded data for graph {idx}:\", data)\n",
    "        return data\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.graph_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = \"../\"\n",
    "test = 'testset/'\n",
    "train = 'trainset/'\n",
    "val = 'valset/'\n",
    "indir = '/users/santoshp/trackml_data/mockdata/'\n",
    "    \n",
    "graph_files_test = np.array(os.listdir(indir + test))\n",
    "graph_files_test = [os.path.join(indir,file)\n",
    "                           for file in graph_files_test]\n",
    "graph_files_train = np.array(os.listdir(indir+train))\n",
    "graph_files_train = [os.path.join(indir, file)\n",
    "                            for file in graph_files_train]\n",
    "graph_files_val = np.array(os.listdir(indir+val))\n",
    "graph_files_val = [os.path.join(indir,file)\n",
    "                            for file in graph_files_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/users/santoshp/trackml_data/mockdata/event0.pyg']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " graph_files_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size': 8, 'shuffle': True, 'num_workers': 0}\n",
    "train_set = GraphDataset(graph_files_train,train)\n",
    "train_loader = DataLoader(train_set,**params)  #batches join graphs instead of splitting them therefore more than train set batches will make 1 batch only \n",
    "test_set = GraphDataset(graph_files_test, test)\n",
    "test_loader = DataLoader(test_set, **params)\n",
    "val_set = GraphDataset(graph_files_val, val)\n",
    "val_loader = DataLoader(val_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data for graph 0: DataBatch(leta=[14524], cell_count=[14524], z=[14524], phi=[14524], module_index=[14524], lphi=[14524], ly=[14524], hit_id=[14524], r=[14524], lz=[14524], cell_val=[14524], y=[187], gphi=[14524], weight=[14524], lx=[14524], eta=[14524], geta=[14524], x=[14524], region=[14524], track_edges=[2, 69], pt=[69], radius=[69], particle_id=[69], nhits=[69], config=[2], event_id=[1], num_nodes=14524, batch=[14524], edge_index=[2, 187], truth_map=[69], weights=[187], scores=[187])\n",
      "DataBatch(leta=[14524], cell_count=[14524], z=[14524], phi=[14524], module_index=[14524], lphi=[14524], ly=[14524], hit_id=[14524], r=[14524], lz=[14524], cell_val=[14524], y=[187], gphi=[14524], weight=[14524], lx=[14524], eta=[14524], geta=[14524], x=[14524], region=[14524], track_edges=[2, 69], pt=[69], radius=[69], particle_id=[69], nhits=[69], config=[2], event_id=[1], num_nodes=14524, batch=[14524], edge_index=[2, 187], truth_map=[69], weights=[187], scores=[187])\n",
      "Loaded data for graph 0: DataBatch(leta=[14524], cell_count=[14524], z=[14524], phi=[14524], module_index=[14524], lphi=[14524], ly=[14524], hit_id=[14524], r=[14524], lz=[14524], cell_val=[14524], y=[187], gphi=[14524], weight=[14524], lx=[14524], eta=[14524], geta=[14524], x=[14524], region=[14524], track_edges=[2, 69], pt=[69], radius=[69], particle_id=[69], nhits=[69], config=[2], event_id=[1], num_nodes=14524, batch=[14524], edge_index=[2, 187], truth_map=[69], weights=[187], scores=[187])\n",
      "DataBatch(leta=[14524], cell_count=[14524], z=[14524], phi=[14524], module_index=[14524], lphi=[14524], ly=[14524], hit_id=[14524], r=[14524], lz=[14524], cell_val=[14524], y=[187], gphi=[14524], weight=[14524], lx=[14524], eta=[14524], geta=[14524], x=[14524], region=[14524], track_edges=[2, 69], pt=[69], radius=[69], particle_id=[69], nhits=[69], config=[1], event_id=[1], num_nodes=14524, batch=[14524], edge_index=[2, 187], truth_map=[69], weights=[187], scores=[187])\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "  print(test_set.get(i))\n",
    "for batch_idx, data in enumerate(test_loader):\n",
    "   print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphDataset()\n"
     ]
    }
   ],
   "source": [
    "print(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelationalModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super(RelationalModel, self).__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, m):\n",
    "        return self.layers(m)\n",
    "\n",
    "class ObjectModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super(ObjectModel, self).__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, C):\n",
    "        return self.layers(C)\n",
    "\n",
    "\n",
    "class InteractionNetwork(MessagePassing):\n",
    "    def __init__(self, node_f_size, edge_attr_size,message_out, update_out, hidden_size):\n",
    "        super(InteractionNetwork, self).__init__(aggr='add', \n",
    "                                                 flow='source_to_target')\n",
    "        self.R1 = RelationalModel(2*node_f_size + edge_attr_size, message_out, hidden_size)    # 19 is the node_features * 2 + edge atributes output 4 \n",
    "        self.O = ObjectModel(node_f_size + message_out, update_out, hidden_size)    # 10 is node features + output R1\n",
    "        self.R2 = RelationalModel(2*update_out + message_out , 1, hidden_size)  #10 is from 2* output O + output R1(from the concat) \n",
    "        self.E: Tensor = Tensor()\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Tensor, edge_attr: Tensor) -> Tensor:\n",
    "        x_tilde = self.propagate(edge_index, x=x, edge_attr=edge_attr, size=None)\n",
    "        m2 = torch.cat([x_tilde[edge_index[1]],\n",
    "                        x_tilde[edge_index[0]],\n",
    "                        self.E], dim=1)\n",
    "        return torch.sigmoid(self.R2(m2))\n",
    "        \n",
    "    def message(self, x_i, x_j, edge_attr):\n",
    "        m1 = torch.cat([x_i, x_j, edge_attr], dim=1)\n",
    "        self.E = self.R1(m1)\n",
    "        return self.E\n",
    "\n",
    "    def update(self, aggr_out, x):\n",
    "        c = torch.cat([x, aggr_out], dim=1)\n",
    "        return self.O(c) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data for graph 0: DataBatch(leta=[14524], cell_count=[14524], z=[14524], phi=[14524], module_index=[14524], lphi=[14524], ly=[14524], hit_id=[14524], r=[14524], lz=[14524], cell_val=[14524], y=[187], gphi=[14524], weight=[14524], lx=[14524], eta=[14524], geta=[14524], x=[14524], region=[14524], track_edges=[2, 69], pt=[69], radius=[69], particle_id=[69], nhits=[69], config=[2], event_id=[1], num_nodes=14524, batch=[14524], edge_index=[2, 187], truth_map=[69], weights=[187], scores=[187])\n",
      "DataBatch(leta=[14524], cell_count=[14524], z=[14524], phi=[14524], module_index=[14524], lphi=[14524], ly=[14524], hit_id=[14524], r=[14524], lz=[14524], cell_val=[14524], y=[187], gphi=[14524], weight=[14524], lx=[14524], eta=[14524], geta=[14524], x=[14524], region=[14524], track_edges=[2, 69], pt=[69], radius=[69], particle_id=[69], nhits=[69], config=[2], event_id=[1], num_nodes=14524, batch=[14524], edge_index=[2, 187], truth_map=[69], weights=[187], scores=[187])\n"
     ]
    }
   ],
   "source": [
    "print(train_set.get(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data for graph 0: DataBatch(leta=[14524], cell_count=[14524], z=[14524], phi=[14524], module_index=[14524], lphi=[14524], ly=[14524], hit_id=[14524], r=[14524], lz=[14524], cell_val=[14524], y=[187], gphi=[14524], weight=[14524], lx=[14524], eta=[14524], geta=[14524], x=[14524], region=[14524], track_edges=[2, 69], pt=[69], radius=[69], particle_id=[69], nhits=[69], config=[2], event_id=[1], num_nodes=14524, batch=[14524], edge_index=[2, 187], truth_map=[69], weights=[187], scores=[187])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/santoshp/.conda/envs/gnn4itk/lib/python3.9/site-packages/torch/nn/init.py:405: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    }
   ],
   "source": [
    "# Define the size of layers in the neural network\n",
    "hidden_l_size = 12    # Tunable parameter\n",
    "message_out = 4       # Tunable parameter\n",
    "update_out = 3        # Tunable parameter\n",
    "\n",
    "# Get the size of edge attributes and node features from the dataset\n",
    "data = train_set.get(0)\n",
    "\n",
    "# Check if edge_attr is not None and has a shape before accessing its shape\n",
    "if data.edge_attr is not None and hasattr(data.edge_attr, 'shape') and len(data.edge_attr.shape) > 1:\n",
    "    edge_attr_size = data.edge_attr.shape[1]\n",
    "else:\n",
    "    # Handle the case when edge_attr is None or doesn't have the expected shape\n",
    "    edge_attr_size = 0  # or any other appropriate value\n",
    "\n",
    "# Similarly, check for the 'x' attribute\n",
    "if data.x is not None and hasattr(data.x, 'shape') and len(data.x.shape) > 1:\n",
    "    node_f_size = data.x.shape[1]\n",
    "else:\n",
    "    # Handle the case when 'x' is None or doesn't have the expected shape\n",
    "    node_f_size = 0  # or any other appropriate value\n",
    "\n",
    "# Initialize the model\n",
    "model = InteractionNetwork(node_f_size=node_f_size, edge_attr_size=edge_attr_size, message_out=message_out, update_out=update_out, hidden_size=hidden_l_size).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(y_pred, y_test,thld):\n",
    "  \"\"\"\n",
    "  returns accuracy based on a given treshold\n",
    "  \"\"\"\n",
    "\n",
    "  # true positives edges with ouput prediction bigger than thld(1) and label = 1\n",
    "  TP = torch.sum((y_test==1.).squeeze() & \n",
    "                           (y_pred>thld).squeeze()).item()\n",
    "  # true negatives edges with ouput prediction smaller than thld(0) and label = 0\n",
    "  TN = torch.sum((y_test==0.).squeeze() & \n",
    "                           (y_pred<thld).squeeze()).item()\n",
    "  # False positives edges with ouput prediction bigger than thld(1) and label = 0\n",
    "  FP = torch.sum((y_test==0.).squeeze() & \n",
    "                           (y_pred>thld).squeeze()).item()\n",
    "  # False negatives edges with ouput prediction smaller than thld(0) and label = 1                     \n",
    "  FN = torch.sum((y_test==1.).squeeze() & \n",
    "                           (y_pred<thld).squeeze()).item() \n",
    "  #how many correct predictions are made, if FP = 0 and FN = 0 acc = 1                       \n",
    "  acc = (TP+TN)/(TP+TN+FP+FN)\n",
    "    \n",
    "  return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total trainable params: 776\n"
     ]
    }
   ],
   "source": [
    "total_trainable_params = sum(p.numel() for p in model.parameters())\n",
    "print('total trainable params:', total_trainable_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "preccision_acc = 0.5\n",
    "def train( model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    epoch_t0 = time()\n",
    "    losses = []\n",
    "    accs = []\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.x, data.edge_index, data.edge_attr)\n",
    "        y, output = data.y, output.squeeze(1)\n",
    "        loss = F.binary_cross_entropy(output, y, reduction='mean')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "       \n",
    "        # if batch_idx % 10 == 0:\n",
    "        #   print(f\"\"\"Train Epoch:{epoch} [{batch_idx}/{len(train_loader.dataset)}({100. * batch_idx / len(train_loader):.0f}%)]\"\"\")\n",
    "        # if batch_idx % 10 == 0:\n",
    "        #     print(f\"\"\"Train Epoch:{epoch}\"\"\")\n",
    "            \n",
    "            # if args.dry_run:\n",
    "            #     break\n",
    "        losses.append(loss.item())   #this is the same as loss because there is no batches \n",
    "        accs.append(binary_acc(y_pred = output, y_test = y, thld = preccision_acc ))\n",
    "\n",
    "    print(f\"...epoch time: {time()-epoch_t0}s\")\n",
    "    print(f\"...epoch {epoch}: mean train loss={np.mean(losses):.6f}......train acc={np.mean(accs):.6f}\")\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    " preccision_acc = 0.5\n",
    "def train( model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    epoch_t0 = time()\n",
    "    losses = []\n",
    "    accs = []\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.x, data.edge_index, data.edge_attr)\n",
    "        y, output = data.y, output.squeeze(1)\n",
    "        loss = F.binary_cross_entropy(output, y, reduction='mean')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "       \n",
    "        # if batch_idx % 10 == 0:\n",
    "        #   print(f\"\"\"Train Epoch:{epoch} [{batch_idx}/{len(train_loader.dataset)}({100. * batch_idx / len(train_loader):.0f}%)]\"\"\")\n",
    "        # if batch_idx % 10 == 0:\n",
    "        #     print(f\"\"\"Train Epoch:{epoch}\"\"\")\n",
    "            \n",
    "            # if args.dry_run:\n",
    "            #     break\n",
    "        losses.append(loss.item())   #this is the same as loss because there is no batches \n",
    "        accs.append(binary_acc(y_pred = output, y_test = y, thld = preccision_acc ))\n",
    "\n",
    "    print(f\"...epoch time: {time()-epoch_t0}s\")\n",
    "    print(f\"...epoch {epoch}: mean train loss={np.mean(losses):.6f}......train acc={np.mean(accs):.6f}\")\n",
    "    return np.mean(losses)\n",
    "\n",
    "def validate(model, device, val_loader):\n",
    "    model.eval()\n",
    "    opt_thlds, accs = [], []\n",
    "    for batch_idx, data in enumerate(val_loader):\n",
    "        data = data.to(device)\n",
    "        output = model(data.x, data.edge_index, data.edge_attr)\n",
    "        y, output = data.y, output.squeeze()\n",
    "        loss = F.binary_cross_entropy(output, y, reduction='mean').item()\n",
    "        \n",
    "        # define optimal threshold (thld) where TPR = TNR \n",
    "        diff, opt_thld, opt_acc = 100, 0, 0\n",
    "        best_tpr, best_tnr = 0, 0\n",
    "        for thld in np.arange(0.001, 0.5, 0.001):\n",
    "            TP = torch.sum((y==1) & (output>thld)).item()\n",
    "            TN = torch.sum((y==0) & (output<thld)).item()\n",
    "            FP = torch.sum((y==0) & (output>thld)).item()\n",
    "            FN = torch.sum((y==1) & (output<thld)).item()\n",
    "            acc = (TP+TN)/(TP+TN+FP+FN)\n",
    "            TPR, TNR = TP/(TP+FN), TN/(TN+FP)\n",
    "            delta = abs(TPR-TNR)\n",
    "            if (delta < diff): \n",
    "                diff, opt_thld, opt_acc = delta, thld, acc\n",
    "\n",
    "        opt_thlds.append(opt_thld)\n",
    "        accs.append(opt_acc)\n",
    "\n",
    "    print(f\"...........................................val acc={np.mean(accs):.6f}\")\n",
    "    return np.mean(opt_thlds) \n",
    "\n",
    "def test(model, device, test_loader, thld=0.5):\n",
    "    model.eval()\n",
    "    test_t0 = time()\n",
    "    losses, accs = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            output = model(data.x, data.edge_index, data.edge_attr)\n",
    "            acc = binary_acc(y_pred = output, y_test = data.y, thld =  thld)\n",
    "            loss = F.binary_cross_entropy(output.squeeze(1), data.y, \n",
    "                                          reduction='mean').item()\n",
    "            accs.append(acc)\n",
    "            losses.append(loss)\n",
    "    \n",
    "    #when batching works change acc for mean accs\n",
    "    print(f\"...testing time: {time()-test_t0}s\")\n",
    "    print(f'.............mean test loss={np.mean(losses):.6f}......test acc ={acc:.6f}\\n')\n",
    "    return np.mean(losses), np.mean(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Epoch 1 ----\n",
      "Loaded data for graph 0: DataBatch(leta=[14524], cell_count=[14524], z=[14524], phi=[14524], module_index=[14524], lphi=[14524], ly=[14524], hit_id=[14524], r=[14524], lz=[14524], cell_val=[14524], y=[187], gphi=[14524], weight=[14524], lx=[14524], eta=[14524], geta=[14524], x=[14524], region=[14524], track_edges=[2, 69], pt=[69], radius=[69], particle_id=[69], nhits=[69], config=[2], event_id=[1], num_nodes=14524, batch=[14524], edge_index=[2, 187], truth_map=[69], weights=[187], scores=[187])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got -2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/users/santoshp/test_gnn/from_alesandra.ipynb Cell 16\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsuperpod/users/santoshp/test_gnn/from_alesandra.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m50\u001b[39m \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsuperpod/users/santoshp/test_gnn/from_alesandra.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m---- Epoch \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m ----\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(epoch))\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bsuperpod/users/santoshp/test_gnn/from_alesandra.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m   train_loss \u001b[39m=\u001b[39m train( model, device, train_loader, optimizer, epoch)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsuperpod/users/santoshp/test_gnn/from_alesandra.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m   thld \u001b[39m=\u001b[39m validate(model, device, val_loader)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsuperpod/users/santoshp/test_gnn/from_alesandra.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m   \u001b[39m#print('...optimal threshold', thld)\u001b[39;00m\n",
      "\u001b[1;32m/users/santoshp/test_gnn/from_alesandra.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsuperpod/users/santoshp/test_gnn/from_alesandra.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsuperpod/users/santoshp/test_gnn/from_alesandra.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bsuperpod/users/santoshp/test_gnn/from_alesandra.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m output \u001b[39m=\u001b[39m model(data\u001b[39m.\u001b[39;49mx, data\u001b[39m.\u001b[39;49medge_index, data\u001b[39m.\u001b[39;49medge_attr)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuperpod/users/santoshp/test_gnn/from_alesandra.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m y, output \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39my, output\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuperpod/users/santoshp/test_gnn/from_alesandra.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mbinary_cross_entropy(output, y, reduction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/gnn4itk/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/users/santoshp/test_gnn/from_alesandra.ipynb Cell 16\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuperpod/users/santoshp/test_gnn/from_alesandra.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor, edge_index: Tensor, edge_attr: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bsuperpod/users/santoshp/test_gnn/from_alesandra.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m     x_tilde \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropagate(edge_index, x\u001b[39m=\u001b[39;49mx, edge_attr\u001b[39m=\u001b[39;49medge_attr, size\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuperpod/users/santoshp/test_gnn/from_alesandra.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m     m2 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([x_tilde[edge_index[\u001b[39m1\u001b[39m]],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuperpod/users/santoshp/test_gnn/from_alesandra.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m                     x_tilde[edge_index[\u001b[39m0\u001b[39m]],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuperpod/users/santoshp/test_gnn/from_alesandra.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m                     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mE], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuperpod/users/santoshp/test_gnn/from_alesandra.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39msigmoid(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mR2(m2))\n",
      "File \u001b[0;32m~/.conda/envs/gnn4itk/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py:429\u001b[0m, in \u001b[0;36mMessagePassing.propagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m decomp_args:\n\u001b[1;32m    427\u001b[0m         kwargs[arg] \u001b[39m=\u001b[39m decomp_kwargs[arg][i]\n\u001b[0;32m--> 429\u001b[0m coll_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__collect__(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__user_args__, edge_index,\n\u001b[1;32m    430\u001b[0m                              size, kwargs)\n\u001b[1;32m    432\u001b[0m msg_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minspector\u001b[39m.\u001b[39mdistribute(\u001b[39m'\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m'\u001b[39m, coll_dict)\n\u001b[1;32m    433\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_message_forward_pre_hooks\u001b[39m.\u001b[39mvalues():\n",
      "File \u001b[0;32m~/.conda/envs/gnn4itk/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py:300\u001b[0m, in \u001b[0;36mMessagePassing.__collect__\u001b[0;34m(self, args, edge_index, size, kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m     data \u001b[39m=\u001b[39m data[dim]\n\u001b[1;32m    299\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, Tensor):\n\u001b[0;32m--> 300\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__set_size__(size, dim, data)\n\u001b[1;32m    301\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__lift__(data, edge_index, dim)\n\u001b[1;32m    303\u001b[0m out[arg] \u001b[39m=\u001b[39m data\n",
      "File \u001b[0;32m~/.conda/envs/gnn4itk/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py:224\u001b[0m, in \u001b[0;36mMessagePassing.__set_size__\u001b[0;34m(self, size, dim, src)\u001b[0m\n\u001b[1;32m    222\u001b[0m the_size \u001b[39m=\u001b[39m size[dim]\n\u001b[1;32m    223\u001b[0m \u001b[39mif\u001b[39;00m the_size \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 224\u001b[0m     size[dim] \u001b[39m=\u001b[39m src\u001b[39m.\u001b[39;49msize(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnode_dim)\n\u001b[1;32m    225\u001b[0m \u001b[39melif\u001b[39;00m the_size \u001b[39m!=\u001b[39m src\u001b[39m.\u001b[39msize(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnode_dim):\n\u001b[1;32m    226\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    227\u001b[0m         (\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEncountered tensor with size \u001b[39m\u001b[39m{\u001b[39;00msrc\u001b[39m.\u001b[39msize(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnode_dim)\u001b[39m}\u001b[39;00m\u001b[39m in \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    228\u001b[0m          \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdimension \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnode_dim\u001b[39m}\u001b[39;00m\u001b[39m, but expected size \u001b[39m\u001b[39m{\u001b[39;00mthe_size\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got -2)"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "output = {'train_loss': [], 'test_loss': [], 'test_acc': []}\n",
    "SAVE_MODEL = False\n",
    "for epoch in range(1, 50 + 1):\n",
    "  print(\"---- Epoch {} ----\".format(epoch))\n",
    "  train_loss = train( model, device, train_loader, optimizer, epoch)\n",
    "  thld = validate(model, device, val_loader)\n",
    "  #print('...optimal threshold', thld)\n",
    "  test_loss, test_acc = test(model, device, test_loader, thld=thld)\n",
    "  scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70104eac-de2f-4d23-ac3c-313f4b2d1e55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Environment",
   "language": "python",
   "name": "gnn4itk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
